{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import argparse\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data.distributed\n",
    "import torch.utils.data\n",
    "import tensorly as tl\n",
    "\n",
    "import utils.common as utils\n",
    "from data import cifar10\n",
    "from models.cifar10.vgg import vgg_16_bn\n",
    "from ptflops import get_model_complexity_info\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.decomposition import parafac\n",
    "tl.set_backend('pytorch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_model = vgg_16_bn().cuda()\n",
    "ckpt = torch.load('./checkpoint/cifar10/vgg_16_bn.pt', map_location='cuda:0')\n",
    "origin_model.load_state_dict(ckpt['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(origin_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    # load training data\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data/cifar-10-batches-py/', train=True, download=True,\n",
    "                                            transform=transform_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=256, shuffle=True, num_workers=2)\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar-10-batches-py/', train=False, download=True, transform=transform_test)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freq = (256*50)//256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_loader, model, criterion, optimizer, scheduler):\n",
    "    losses = utils.AverageMeter('Loss', ':.4e')\n",
    "    top1 = utils.AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = utils.AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        cur_lr = param_group['lr']\n",
    "    print('learning_rate: ' + str(cur_lr))\n",
    "\n",
    "    num_iter = len(train_loader)\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # compute outputy\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        n = images.size(0)\n",
    "        losses.update(loss.item(), n)  # accumulated loss\n",
    "        top1.update(prec1.item(), n)\n",
    "        top5.update(prec5.item(), n)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print(\n",
    "                'Epoch[{0}]({1}/{2}): '\n",
    "                'Loss {loss.avg:.4f} '\n",
    "                'Prec@1(1,5) {top1.avg:.2f}, {top5.avg:.2f} '\n",
    "                'Lr {cur_lr:.4f}'.format(\n",
    "                    epoch, i, num_iter, loss=losses,\n",
    "                    top1=top1, top5=top5, cur_lr=cur_lr))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    losses = utils.AverageMeter('Loss', ':.4e')\n",
    "    top1 = utils.AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = utils.AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            pred1, pred5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "            n = images.size(0)\n",
    "            losses.update(loss.item(), n)\n",
    "            top1.update(pred1[0], n)\n",
    "            top5.update(pred5[0], n)\n",
    "\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "_, ori_acc, _ = validate(val_loader, origin_model, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerC(nn.Module):\n",
    "    def __init__(self, C, padding):\n",
    "        super(LayerC, self).__init__()\n",
    "        self.C = nn.Parameter(torch.Tensor(C))\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Add padding to input\n",
    "        batch_size, Cin, h, w = input.shape\n",
    "        padded_I = nn.functional.pad(input, [self.padding]*4)\n",
    "        padded_I = padded_I.permute(0, 2, 3, 1)\n",
    "        device = input.device\n",
    "        Cout, _, r = self.C.shape\n",
    "        # Calculate output size after padding\n",
    "        padded_h = h + 2 * self.padding\n",
    "        padded_w = w + 2 * self.padding\n",
    "\n",
    "        # Step 1: Compute Oc\n",
    "        padded_I_col = padded_I.reshape(batch_size * padded_h * padded_w, Cin)\n",
    "        C_col = self.C.permute(1, 0, 2).reshape(Cin, Cout * r)\n",
    "\n",
    "        # Compute matrix multiplication and reshape output\n",
    "        output = torch.matmul(padded_I_col, C_col).reshape(\n",
    "            batch_size, padded_h, padded_w, Cout, r)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerB(nn.Module):\n",
    "    def __init__(self, B, padding):\n",
    "        super(LayerB, self).__init__()\n",
    "        self.B = nn.Parameter(torch.Tensor(B))\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input):\n",
    "        device = input.device\n",
    "        _, padded_h, padded_w, Cout, r = input.shape\n",
    "        Cout, d, _ = self.B.shape\n",
    "        w = padded_h - 2 * self.padding\n",
    "\n",
    "        # Step 2: Compute Ob\n",
    "        Oc = input.permute(0, 1, 3, 4, 2)\n",
    "        B = self.B.permute(0, 2, 1)\n",
    "\n",
    "        # Add a new axis to B for broadcasting, B's shape becomes (1, 1, Cout, r, 1, d)\n",
    "        B_expanded = B[None, None, :, :, None, :]\n",
    "        # Assuming 'Oc' is a 5-dimensional and 'w' and 'd' are the window width and depth, respectively\n",
    "        window_indices = torch.arange(w)[:, None] + torch.arange(d)\n",
    "        Oc_expanded = Oc[:, :, :, :, window_indices]\n",
    "\n",
    "        # Perform the element-wise multiplication and sum over the last axis (d)\n",
    "        output = torch.sum(Oc_expanded * B_expanded, dim=-1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerA(nn.Module):\n",
    "    def __init__(self, A, padding, bias):\n",
    "        super(LayerA, self).__init__()\n",
    "        self.A = nn.Parameter(torch.Tensor(A))\n",
    "        self.padding = padding\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        device = input.device\n",
    "        _, padded_h, Cout, r, w = input.shape\n",
    "        Cout, d, _ = self.A.shape\n",
    "\n",
    "        h = padded_h - 2 * self.padding\n",
    "\n",
    "        # Step 3: Compute Oa\n",
    "        A = self.A.permute(0, 2, 1)\n",
    "        Ob = input.permute(0, 4, 2, 3, 1)\n",
    "\n",
    "        # Add a new axis to B for broadcasting, A's shape becomes (1, 1, Cout, r, 1, d)\n",
    "        A_expanded = A[None, None, :, :, None, :]\n",
    "        # Assuming 'Ob' is a 5-dimensional and 'h' and 'd' are the window width and depth, respectively\n",
    "\n",
    "        window_indices = torch.arange(h)[:, None] + torch.arange(d)\n",
    "        Ob_expanded = Ob[:, :, :, :, window_indices]\n",
    "\n",
    "        # Perform the element-wise multiplication and sum over the last axis (d)\n",
    "        Oa = torch.sum(Ob_expanded * A_expanded, axis=-1)\n",
    "\n",
    "        Oa = Oa.permute(0, 4, 1, 2, 3)\n",
    "\n",
    "        # Step 4: Compute O\n",
    "        output = torch.sum(Oa, dim=-1) + self.bias\n",
    "\n",
    "        output = output.permute(0, 3, 1, 2)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decomposition.CPDLayers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_decomposition_conv_layer(layer, rank):\n",
    "    \"\"\" Gets a conv layer and a target rank, \n",
    "        returns a nn.Sequential object with the decomposition \"\"\"\n",
    "\n",
    "    padding = layer.padding[0]\n",
    "    kernel_size = layer.kernel_size[0]\n",
    "    Cin = layer.in_channels\n",
    "    Cout = layer.out_channels\n",
    "    W = layer.weight.data\n",
    "    device = W.get_device()\n",
    "\n",
    "    # Initialize the factor matrices with zeros\n",
    "    body_factors = torch.zeros(Cout, kernel_size, rank, device=device)\n",
    "    tail_factors = torch.zeros(Cout, kernel_size, rank, device=device)\n",
    "    head_factors = torch.zeros(Cout, Cin, rank, device=device)\n",
    "\n",
    "    for i in tqdm(range(Cout)):\n",
    "        head_factors[i], tail_factors[i], body_factors[i] = parafac(\n",
    "            W[i, :, :, :], rank=rank, n_iter_max=1000, tol=1e-32, init='random')[1]\n",
    "        factors = parafac(W[i, :, :, :], rank=rank, n_iter_max=1000, tol=1e-32, init='random')\n",
    "        resconstructed = tl.cp_to_tensor(factors)\n",
    "        print((torch.norm(W[i, :, :, :]-resconstructed, p=2).item()/torch.norm(W[i, :, :, :], p=2).item())**2)\n",
    "    # head = LayerC(head_factors, padding)\n",
    "    # body = LayerB(body_factors, padding)\n",
    "    # tail = LayerA(tail_factors, padding, layer.bias.data)\n",
    "    assert not torch.isnan(head_factors).any(\n",
    "    ), \"head_factors tensor from parafac is nan\"\n",
    "    assert not torch.isnan(body_factors).any(\n",
    "    ), \"body_factors tensor from parafac is nan\"\n",
    "    assert not torch.isnan(tail_factors).any(\n",
    "    ), \"tail_factors tensor from parafac is nan\"\n",
    "\n",
    "    head_factors = head_factors.permute(1, 0, 2)\n",
    "    body_factors = body_factors.permute(0, 2, 1)\n",
    "    tail_factors = tail_factors.permute(0, 2, 1)\n",
    "    head = CPDHead(Cin, Cout, rank, padding, head_factors)\n",
    "    body = CPDBody(Cin, Cout, rank, kernel_size, padding, body_factors)\n",
    "    tail = CPDTail(Cin, Cout, rank, kernel_size, padding,\n",
    "                   tail_factors, layer.bias.data)\n",
    "\n",
    "    new_layers = [head, body, tail]\n",
    "\n",
    "    return nn.Sequential(*new_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = torch.randn(128, 64, 3, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_factors = torch.randn(128, 64, 9, device=0)\n",
    "# body_factors = torch.randn(128, 3, 9, device=0)\n",
    "# tail_factors = torch.randn(128, 3, 9, device=0)\n",
    "# bias = torch.randn([128], device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(128)):\n",
    "#         head_factors[i], _, _ = parafac(W[i, :, :, :], rank=9, n_iter_max=1000, tol= 1e-32, init='random')[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = LayerC(head_factors, 1)\n",
    "# body = LayerB(body_factors, 1)\n",
    "# tail = LayerA(tail_factors, 1, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.randn((1, 64, 32, 32), device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail(body(head(input)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_head = CPDHead(64, 128, 9, 1, head_factors)\n",
    "# new_body = CPDBody(64, 128, 9, 3, 1, body_factors)\n",
    "# new_tail = CPDTail(64, 128, 9, 3, 1, tail_factors, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tail(new_body(new_head(input)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ori, backtracking\n",
    "# def cp_decompose_model(model, exclude_first_conv=True, exclude_linears=True, passed_first_conv=False):\n",
    "#     for name, module in model._modules.items():\n",
    "#         if len(list(module.children())) > 0:\n",
    "#             # recurse\n",
    "#             model._modules[name] = cp_decompose_model(\n",
    "#                 module, exclude_first_conv, exclude_linears, passed_first_conv)\n",
    "#         elif type(module) == nn.Conv2d:\n",
    "#             if passed_first_conv is False:\n",
    "#                 passed_first_conv = True\n",
    "#                 if exclude_first_conv is True:\n",
    "#                     continue\n",
    "\n",
    "#             # if (name == 'conv3'):\n",
    "#             print('name ', name)\n",
    "#             conv_layer = module\n",
    "#             rank = 9\n",
    "\n",
    "#             decomposed = cp_decomposition_conv_layer(conv_layer, rank)\n",
    "\n",
    "#             model._modules[name] = decomposed\n",
    "\n",
    "#             print(model)\n",
    "\n",
    "#             _, decomposed_acc, _ = validate(val_loader, model, criterion)\n",
    "\n",
    "#             print(decomposed_acc)\n",
    "\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_decompose_model(model, exclude_first_conv=True, exclude_linears=True, passed_first_conv=False):\n",
    "    for high_name, high_module in model._modules.items():\n",
    "        # print(high_name, high_module)\n",
    "        if high_name == 'features':\n",
    "            for name, module in high_module._modules.items():\n",
    "                if type(module) == nn.Conv2d:\n",
    "                    if passed_first_conv is False:\n",
    "                        passed_first_conv = True\n",
    "                        if exclude_first_conv is True:\n",
    "                            continue\n",
    "\n",
    "                    # if name == 'conv15':# or name == 'conv4' or name == 'conv6':\n",
    "                    print('name ', name)\n",
    "                    conv_layer = module\n",
    "                    rank = 1\n",
    "\n",
    "                    decomposed = cp_decomposition_conv_layer(conv_layer, rank)\n",
    "\n",
    "                    # model._modules[name] = decomposed\n",
    "                    model._modules[high_name]._modules[name] = decomposed\n",
    "\n",
    "                    # print(model)\n",
    "\n",
    "                    _, decomposed_acc, _ = validate(\n",
    "                        val_loader, model, criterion)\n",
    "\n",
    "                    # print(decomposed_acc)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(origin_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cp_decompose_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(\n",
    "), lr=0.01, momentum=0.9, weight_decay=5e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, div_factor=10, epochs=100, steps_per_epoch=len(\n",
    "    train_loader), pct_start=0.1, final_div_factor=100)\n",
    "\n",
    "start_epoch = 0\n",
    "best_top1_acc = 0\n",
    "\n",
    "# train the model\n",
    "epoch = start_epoch\n",
    "while epoch < 1:\n",
    "    train(epoch,  train_loader, model, criterion, optimizer, scheduler)\n",
    "    _, valid_top1_acc, valid_top5_acc = validate(\n",
    "        val_loader, model, criterion)\n",
    "\n",
    "    is_best = False\n",
    "    if valid_top1_acc > best_top1_acc:\n",
    "        best_top1_acc = valid_top1_acc\n",
    "        is_best = True\n",
    "\n",
    "    cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print('epoch', epoch, 'best_acc', max(\n",
    "        valid_top1_acc, best_top1_acc), 'top1', valid_top1_acc)\n",
    "\n",
    "    epoch += 1\n",
    "    print(\"=>Best accuracy {:.3f}\".format(best_top1_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = origin_model.features.conv14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcp_layer = cp_decomposition_conv_layer(conv, rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_inp = torch.randn((1, 512, 2, 2), device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv)\n",
    "print(dcp_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_output = conv(rand_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_output = dcp_layer(rand_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.norm(ori_output-my_output, p=2).item()/torch.norm(ori_output, p=2).item())**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(ori_output-my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_parameters(origin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchprofile import profile_macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_macs(model, inputs) -> int:\n",
    "    return profile_macs(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 32, 32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_macs(model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_macs(origin_model, dummy_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
